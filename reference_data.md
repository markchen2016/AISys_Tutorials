###### 系列文章
1. 《Transformers快速入门》 https://transformers.run/  
**Abstract** TODO
**阅读状态** doing

2. 《探秘Transformer系列》 https://www.cnblogs.com/rossiXYZ/p/18785601
* 《注意力机制》https://www.cnblogs.com/rossiXYZ/p/18705809
**Abstract** 看完忘写了摘要了...
**阅读状态** 需要重看

* 《Transformer总体架构》 https://www.cnblogs.com/rossiXYZ/p/18706134
**Abstract** 介绍了Transformer中总体的结构，包括Encoder、Decoder中Attention的实现、FFN的实现。包含Transformer的实现代码，后面对Transformer多方面的解释感觉看起来有点费劲。
**阅读状态** done



###### 预训练技术  
1. 《从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史》 https://zhuanlan.zhihu.com/p/49271699  
**Abstract** 介绍了自然语言处理中的预训练技术的发展，其中主要介绍了 Word2Vec（获取词向量）、ELMO（新增双向LSTM，给词向量新增上下文语义特征、句法特征，解决多义词相同embedding问题，双向语言模型）、GPT-1（预训练+下游任务Fine-tuning两阶段，使用Transformer特征抽取器，单向语言模型）、Bert（使用Transformer特征抽取、通过mask根据上下文猜词实现双向语义特征提取，双向语言模型）  
**阅读状态** done
2. 《》